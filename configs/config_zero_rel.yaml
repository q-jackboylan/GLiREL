# Learning Rate
lr_encoder: 1e-5
lr_others: 1e-4

# Training Parameters
num_steps: 200000
warmup_ratio: 0.1
train_batch_size: 1
eval_every: 3000
gradient_accumulation: 8   # null
eval_batch_size: 32

# Model Configuration
max_width: 12
model_name: microsoft/deberta-v3-large # hugging face model
fine_tune: true
subtoken_pooling: first
hidden_size: 768
scorer: "dot" # "dot_norm" # "dot"
span_mode: marker
refine_prompt: false
refine_relation: false
ffn_mul: 4
dropout: 0.4
scheduler: "cosine_with_warmup"  # "cosine_with_hard_restarts"
loss_func: "binary_cross_entropy_loss"  # "binary_cross_entropy_loss"  # "focal_loss"
# alpha: 0.25
# gamma: 2

# Directory Paths
dataset_name: "zero_rel"
root_dir: ablation_backbone
train_data: "data/zero_rel_all.jsonl"

# "none" if no pretrained model 
prev_path: "none"


# Training Specifics
size_sup: -1
num_train_rel_types: 25   # number of relation labels to use in each given mini-batch
num_unseen_rel_types: 15   # 15
top_k: 1                  # number of relations predictions to return at evaluation time
random_drop: false       # randomly drop relation types
max_len: 384
eval_threshold: 0.001

name: "large"


